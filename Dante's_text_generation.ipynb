{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Dante's_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvP3WEqrHN1"
      },
      "source": [
        "# Load data and imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHAjEjgjy9r"
      },
      "source": [
        "In this section, we just clone the repository and load the dataset in a DataFrame splitting the dataset by \"Canti\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGILrolhfbar",
        "outputId": "9ae3bc22-cc4c-4c4b-86c4-4054e05fef90"
      },
      "source": [
        "!git clone https://github.com/FrancescoFarinola/divine-comedy-text-generation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fatal: destination path 'divine-comedy-text-generation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "61IwnQDkjV5r",
        "outputId": "f1163aeb-e123-4b85-8fe2-767c51ca3f28"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sys.path.append('/content/divine-comedy-text-generation')\n",
        "\n",
        "path = 'divine-comedy-text-generation/'\n",
        "filenames = ['inferno.txt', 'purgatorio.txt', 'paradiso.txt']\n",
        "\n",
        "columns =  ['Cantica', 'Canto', 'Text']\n",
        "data = []\n",
        "for file in filenames:\n",
        "  filepath = path + file\n",
        "  with open(filepath, 'r', encoding='utf8') as f:\n",
        "    lines = f.readlines()\n",
        "    start = True\n",
        "    for line in lines:\n",
        "      if 'Canto' in line:\n",
        "        if start: \n",
        "          row = np.empty(3, dtype=object)\n",
        "          start = False\n",
        "        else:\n",
        "          row[2] = canto\n",
        "          data.append(row)\n",
        "          row = np.empty(3, dtype=object)\n",
        "        head = line.split()\n",
        "        row[0] = head[0]\n",
        "        row[1] = head[3]\n",
        "        canto = \"\"\n",
        "      elif line != \"\\n\":\n",
        "        canto = canto + line\n",
        "    row[2] = canto  \n",
        "    data.append(row)\n",
        "    f.close()\n",
        "\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Cantica   Canto                                               Text\n",
              "0    Inferno       I    Nel mezzo del cammin di nostra vita\\n  mi ri...\n",
              "1    Inferno      II    Lo giorno se n’andava, e l’aere bruno\\n  tog...\n",
              "2    Inferno     III    “Per me si va ne la città dolente,\\n  per me...\n",
              "3    Inferno      IV    Ruppemi l’alto sonno ne la testa\\n  un greve...\n",
              "4    Inferno       V    Così discesi del cerchio primaio\\n  giù nel ...\n",
              "..       ...     ...                                                ...\n",
              "95  Paradiso    XXIX    Quando ambedue li figli di Latona,\\n  copert...\n",
              "96  Paradiso     XXX    Forse semilia miglia di lontano\\n  ci ferve ...\n",
              "97  Paradiso    XXXI    In forma dunque di candida rosa\\n  mi si mos...\n",
              "98  Paradiso   XXXII    Affetto al suo piacer, quel contemplante\\n  ...\n",
              "99  Paradiso  XXXIII    «Vergine Madre, figlia del tuo figlio,\\n  um...\n",
              "\n",
              "[100 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cantica</th>\n",
              "      <th>Canto</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Inferno</td>\n",
              "      <td>I</td>\n",
              "      <td>Nel mezzo del cammin di nostra vita\\n  mi ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Inferno</td>\n",
              "      <td>II</td>\n",
              "      <td>Lo giorno se n’andava, e l’aere bruno\\n  tog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Inferno</td>\n",
              "      <td>III</td>\n",
              "      <td>“Per me si va ne la città dolente,\\n  per me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Inferno</td>\n",
              "      <td>IV</td>\n",
              "      <td>Ruppemi l’alto sonno ne la testa\\n  un greve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Inferno</td>\n",
              "      <td>V</td>\n",
              "      <td>Così discesi del cerchio primaio\\n  giù nel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Paradiso</td>\n",
              "      <td>XXIX</td>\n",
              "      <td>Quando ambedue li figli di Latona,\\n  copert...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Paradiso</td>\n",
              "      <td>XXX</td>\n",
              "      <td>Forse semilia miglia di lontano\\n  ci ferve ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Paradiso</td>\n",
              "      <td>XXXI</td>\n",
              "      <td>In forma dunque di candida rosa\\n  mi si mos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Paradiso</td>\n",
              "      <td>XXXII</td>\n",
              "      <td>Affetto al suo piacer, quel contemplante\\n  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Paradiso</td>\n",
              "      <td>XXXIII</td>\n",
              "      <td>«Vergine Madre, figlia del tuo figlio,\\n  um...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8hB8wgvUqwV",
        "outputId": "625b9d10-754b-4c31-b12a-4d60038547a3"
      },
      "source": [
        "df.Text[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Nel mezzo del cammin di nostra vita\\n  mi ritrovai per una selva oscura,\\n  ché la diritta via era smarrita.\\n  Ahi quanto a dir qual era è cosa dura\\n  esta selva selvaggia e aspra e forte\\n  che nel pensier rinova la paura!\\n  Tant’ è amara che poco è più morte;\\n  ma per trattar del ben ch’i’ vi trovai,\\n  dirò de l’altre cose ch’i’ v’ho scorte.\\n  Io non so ben ridir com’ i’ v’intrai,\\n  tant’ era pien di sonno a quel punto\\n  che la verace via abbandonai.\\n  Ma poi ch’i’ fui al piè d’un colle giunto,\\n  là dove terminava quella valle\\n  che m’avea di paura il cor compunto,\\n  guardai in alto e vidi le sue spalle\\n  vestite già de’ raggi del pianeta\\n  che mena dritto altrui per ogne calle.\\n  Allor fu la paura un poco queta,\\n  che nel lago del cor m’era durata\\n  la notte ch’i’ passai con tanta pieta.\\n  E come quei che con lena affannata,\\n  uscito fuor del pelago a la riva,\\n  si volge a l’acqua perigliosa e guata,\\n  così l’animo mio, ch’ancor fuggiva,\\n  si volse a retro a rimirar lo passo\\n  che non lasciò già mai persona viva.\\n  Poi ch’èi posato un poco il corpo lasso,\\n  ripresi via per la piaggia diserta,\\n  sì che ’l piè fermo sempre era ’l più basso.\\n  Ed ecco, quasi al cominciar de l’erta,\\n  una lonza leggera e presta molto,\\n  che di pel macolato era coverta;\\n  e non mi si partia dinanzi al volto,\\n  anzi ’mpediva tanto il mio cammino,\\n  ch’i’ fui per ritornar più volte vòlto.\\n  Temp’ era dal principio del mattino,\\n  e ’l sol montava ’n sù con quelle stelle\\n  ch’eran con lui quando l’amor divino\\n  mosse di prima quelle cose belle;\\n  sì ch’a bene sperar m’era cagione\\n  di quella fiera a la gaetta pelle\\n  l’ora del tempo e la dolce stagione;\\n  ma non sì che paura non mi desse\\n  la vista che m’apparve d’un leone.\\n  Questi parea che contra me venisse\\n  con la test’ alta e con rabbiosa fame,\\n  sì che parea che l’aere ne tremesse.\\n  Ed una lupa, che di tutte brame\\n  sembiava carca ne la sua magrezza,\\n  e molte genti fé già viver grame,\\n  questa mi porse tanto di gravezza\\n  con la paura ch’uscia di sua vista,\\n  ch’io perdei la speranza de l’altezza.\\n  E qual è quei che volontieri acquista,\\n  e giugne ’l tempo che perder lo face,\\n  che ’n tutti suoi pensier piange e s’attrista;\\n  tal mi fece la bestia sanza pace,\\n  che, venendomi ’ncontro, a poco a poco\\n  mi ripigneva là dove ’l sol tace.\\n  Mentre ch’i’ rovinava in basso loco,\\n  dinanzi a li occhi mi si fu offerto\\n  chi per lungo silenzio parea fioco.\\n  Quando vidi costui nel gran diserto,\\n  «Miserere di me», gridai a lui,\\n  «qual che tu sii, od ombra od omo certo!».\\n  Rispuosemi: «Non omo, omo già fui,\\n  e li parenti miei furon lombardi,\\n  mantoani per patrïa ambedui.\\n  Nacqui sub Iulio, ancor che fosse tardi,\\n  e vissi a Roma sotto ’l buono Augusto\\n  nel tempo de li dèi falsi e bugiardi.\\n  Poeta fui, e cantai di quel giusto\\n  figliuol d’Anchise che venne di Troia,\\n  poi che ’l superbo Ilïón fu combusto.\\n  Ma tu perché ritorni a tanta noia?\\n  perché non sali il dilettoso monte\\n  ch’è principio e cagion di tutta gioia?».\\n  «Or se’ tu quel Virgilio e quella fonte\\n  che spandi di parlar sì largo fiume?»,\\n  rispuos’ io lui con vergognosa fronte.\\n  «O de li altri poeti onore e lume,\\n  vagliami ’l lungo studio e ’l grande amore\\n  che m’ha fatto cercar lo tuo volume.\\n  Tu se’ lo mio maestro e ’l mio autore,\\n  tu se’ solo colui da cu’ io tolsi\\n  lo bello stilo che m’ha fatto onore.\\n  Vedi la bestia per cu’ io mi volsi;\\n  aiutami da lei, famoso saggio,\\n  ch’ella mi fa tremar le vene e i polsi».\\n  «A te convien tenere altro vïaggio»,\\n  rispuose, poi che lagrimar mi vide,\\n  «se vuo’ campar d’esto loco selvaggio;\\n  ché questa bestia, per la qual tu gride,\\n  non lascia altrui passar per la sua via,\\n  ma tanto lo ’mpedisce che l’uccide;\\n  e ha natura sì malvagia e ria,\\n  che mai non empie la bramosa voglia,\\n  e dopo ’l pasto ha più fame che pria.\\n  Molti son li animali a cui s’ammoglia,\\n  e più saranno ancora, infin che ’l veltro\\n  verrà, che la farà morir con doglia.\\n  Questi non ciberà terra né peltro,\\n  ma sapïenza, amore e virtute,\\n  e sua nazion sarà tra feltro e feltro.\\n  Di quella umile Italia fia salute\\n  per cui morì la vergine Cammilla,\\n  Eurialo e Turno e Niso di ferute.\\n  Questi la caccerà per ogne villa,\\n  fin che l’avrà rimessa ne lo ’nferno,\\n  là onde ’nvidia prima dipartilla.\\n  Ond’ io per lo tuo me’ penso e discerno\\n  che tu mi segui, e io sarò tua guida,\\n  e trarrotti di qui per loco etterno;\\n  ove udirai le disperate strida,\\n  vedrai li antichi spiriti dolenti,\\n  ch’a la seconda morte ciascun grida;\\n  e vederai color che son contenti\\n  nel foco, perché speran di venire\\n  quando che sia a le beate genti.\\n  A le quai poi se tu vorrai salire,\\n  anima fia a ciò più di me degna:\\n  con lei ti lascerò nel mio partire;\\n  ché quello imperador che là sù regna,\\n  perch’ i’ fu’ ribellante a la sua legge,\\n  non vuol che ’n sua città per me si vegna.\\n  In tutte parti impera e quivi regge;\\n  quivi è la sua città e l’alto seggio:\\n  oh felice colui cu’ ivi elegge!».\\n  E io a lui: «Poeta, io ti richeggio\\n  per quello Dio che tu non conoscesti,\\n  acciò ch’io fugga questo male e peggio,\\n  che tu mi meni là dov’ or dicesti,\\n  sì ch’io veggia la porta di san Pietro\\n  e color cui tu fai cotanto mesti».\\n  Allor si mosse, e io li tenni dietro.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4g7PMAdrPxB"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anxfg_iaz1OT"
      },
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import string\n",
        "\n",
        "\n",
        "PUNCTUATION_RE = re.compile(\"[-—!?:;,.«»“”]\")\n",
        "\n",
        "\n",
        "def clean_start(text):\n",
        "    return text.replace(\"  \", \"\", 1)\n",
        "\n",
        "def remove_double_whitespaces(text):\n",
        "    return text.replace(\"  \", \"\")\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return PUNCTUATION_RE.sub(\"\", text)\n",
        "\n",
        "def clean_newline(text):\n",
        "    return text.replace(\"\\n\", \" \\n \")\n",
        "\n",
        "def space_apostrophe(text):\n",
        "    return text.replace(\"’\", \"'\")\n",
        "\n",
        "def replace_uncommon_symbols(text):\n",
        "    \"\"\"\n",
        "    Replace uncomoon symbols with particular accents\n",
        "    \"\"\"\n",
        "    text = text.replace(\"ä\", \"a\")\n",
        "    text = text.replace(\"é\", \"è\")\n",
        "    text = text.replace(\"ë\", \"è\")\n",
        "    text = text.replace(\"Ë\", \"E\")\n",
        "    text = text.replace(\"ï\", \"i\")\n",
        "    text = text.replace(\"Ï\", \"I\")\n",
        "    text = text.replace(\"ó\", \"ò\")\n",
        "    text = text.replace(\"ö\", \"o\")\n",
        "    text = text.replace(\"ü\", \"u\")\n",
        "    text = text.replace(\"(\", \"-\")\n",
        "    text = text.replace(\")\", \"-\")\n",
        "    text = text.replace(\"[\", \"\")\n",
        "    text = text.replace(\"]\", \"\")\n",
        "    text = text.replace(\"ï\", \"i\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def adjust_newline(text):\n",
        "    return text.replace(\"\\n \", \"\\n\")\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    return reduce(lambda text, f: f(text), PREPROCESSING_PIPELINE, text)\n",
        "\n",
        "\n",
        "PREPROCESSING_PIPELINE = [remove_double_whitespaces,\n",
        "                          remove_punctuation,\n",
        "                          remove_double_whitespaces,\n",
        "                          space_apostrophe,\n",
        "                          replace_uncommon_symbols,\n",
        "                          clean_newline]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcW7LVAU2pLe",
        "outputId": "05b7ac21-0341-4d47-a22c-6586632201a3"
      },
      "source": [
        "df['Text'] = df['Text'].apply(lambda x: preprocessing(x))\n",
        "df['Text'][0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nel mezzo del cammin di nostra vita \\n mi ritrovai per una selva oscura \\n chè la diritta via era smarrita \\n Ahi quanto a dir qual era è cosa dura \\n esta selva selvaggia e aspra e forte \\n che nel pensier rinova la paura \\n Tant' è amara che poco è più morte \\n ma per trattar del ben ch'i' vi trovai \\n dirò de l'altre cose ch'i' v'ho scorte \\n Io non so ben ridir com' i' v'intrai \\n tant' era pien di sonno a quel punto \\n che la verace via abbandonai \\n Ma poi ch'i' fui al piè d'un colle giunto \\n là dove terminava quella valle \\n che m'avea di paura il cor compunto \\n guardai in alto e vidi le sue spalle \\n vestite già de' raggi del pianeta \\n che mena dritto altrui per ogne calle \\n Allor fu la paura un poco queta \\n che nel lago del cor m'era durata \\n la notte ch'i' passai con tanta pieta \\n E come quei che con lena affannata \\n uscito fuor del pelago a la riva \\n si volge a l'acqua perigliosa e guata \\n così l'animo mio ch'ancor fuggiva \\n si volse a retro a rimirar lo passo \\n che non lasciò già mai persona viva \\n Poi ch'èi posato un poco il corpo lasso \\n ripresi via per la piaggia diserta \\n sì che 'l piè fermo sempre era 'l più basso \\n Ed ecco quasi al cominciar de l'erta \\n una lonza leggera e presta molto \\n che di pel macolato era coverta \\n e non mi si partia dinanzi al volto \\n anzi 'mpediva tanto il mio cammino \\n ch'i' fui per ritornar più volte vòlto \\n Temp' era dal principio del mattino \\n e 'l sol montava 'n sù con quelle stelle \\n ch'eran con lui quando l'amor divino \\n mosse di prima quelle cose belle \\n sì ch'a bene sperar m'era cagione \\n di quella fiera a la gaetta pelle \\n l'ora del tempo e la dolce stagione \\n ma non sì che paura non mi desse \\n la vista che m'apparve d'un leone \\n Questi parea che contra me venisse \\n con la test' alta e con rabbiosa fame \\n sì che parea che l'aere ne tremesse \\n Ed una lupa che di tutte brame \\n sembiava carca ne la sua magrezza \\n e molte genti fè già viver grame \\n questa mi porse tanto di gravezza \\n con la paura ch'uscia di sua vista \\n ch'io perdei la speranza de l'altezza \\n E qual è quei che volontieri acquista \\n e giugne 'l tempo che perder lo face \\n che 'n tutti suoi pensier piange e s'attrista \\n tal mi fece la bestia sanza pace \\n che venendomi 'ncontro a poco a poco \\n mi ripigneva là dove 'l sol tace \\n Mentre ch'i' rovinava in basso loco \\n dinanzi a li occhi mi si fu offerto \\n chi per lungo silenzio parea fioco \\n Quando vidi costui nel gran diserto \\n Miserere di me gridai a lui \\n qual che tu sii od ombra od omo certo \\n Rispuosemi Non omo omo già fui \\n e li parenti miei furon lombardi \\n mantoani per patria ambedui \\n Nacqui sub Iulio ancor che fosse tardi \\n e vissi a Roma sotto 'l buono Augusto \\n nel tempo de li dèi falsi e bugiardi \\n Poeta fui e cantai di quel giusto \\n figliuol d'Anchise che venne di Troia \\n poi che 'l superbo Iliòn fu combusto \\n Ma tu perchè ritorni a tanta noia \\n perchè non sali il dilettoso monte \\n ch'è principio e cagion di tutta gioia \\n Or se' tu quel Virgilio e quella fonte \\n che spandi di parlar sì largo fiume \\n rispuos' io lui con vergognosa fronte \\n O de li altri poeti onore e lume \\n vagliami 'l lungo studio e 'l grande amore \\n che m'ha fatto cercar lo tuo volume \\n Tu se' lo mio maestro e 'l mio autore \\n tu se' solo colui da cu' io tolsi \\n lo bello stilo che m'ha fatto onore \\n Vedi la bestia per cu' io mi volsi \\n aiutami da lei famoso saggio \\n ch'ella mi fa tremar le vene e i polsi \\n A te convien tenere altro viaggio \\n rispuose poi che lagrimar mi vide \\n se vuo' campar d'esto loco selvaggio \\n chè questa bestia per la qual tu gride \\n non lascia altrui passar per la sua via \\n ma tanto lo 'mpedisce che l'uccide \\n e ha natura sì malvagia e ria \\n che mai non empie la bramosa voglia \\n e dopo 'l pasto ha più fame che pria \\n Molti son li animali a cui s'ammoglia \\n e più saranno ancora infin che 'l veltro \\n verrà che la farà morir con doglia \\n Questi non ciberà terra nè peltro \\n ma sapienza amore e virtute \\n e sua nazion sarà tra feltro e feltro \\n Di quella umile Italia fia salute \\n per cui morì la vergine Cammilla \\n Eurialo e Turno e Niso di ferute \\n Questi la caccerà per ogne villa \\n fin che l'avrà rimessa ne lo 'nferno \\n là onde 'nvidia prima dipartilla \\n Ond' io per lo tuo me' penso e discerno \\n che tu mi segui e io sarò tua guida \\n e trarrotti di qui per loco etterno \\n ove udirai le disperate strida \\n vedrai li antichi spiriti dolenti \\n ch'a la seconda morte ciascun grida \\n e vederai color che son contenti \\n nel foco perchè speran di venire \\n quando che sia a le beate genti \\n A le quai poi se tu vorrai salire \\n anima fia a ciò più di me degna \\n con lei ti lascerò nel mio partire \\n chè quello imperador che là sù regna \\n perch' i' fu' ribellante a la sua legge \\n non vuol che 'n sua città per me si vegna \\n In tutte parti impera e quivi regge \\n quivi è la sua città e l'alto seggio \\n oh felice colui cu' ivi elegge \\n E io a lui Poeta io ti richeggio \\n per quello Dio che tu non conoscesti \\n acciò ch'io fugga questo male e peggio \\n che tu mi meni là dov' or dicesti \\n sì ch'io veggia la porta di san Pietro \\n e color cui tu fai cotanto mesti \\n Allor si mosse e io li tenni dietro \\n \""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YnxMAIjrV62"
      },
      "source": [
        "# Markov Chain text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm8o-3CQWEcp",
        "outputId": "7fdc6e28-6b0f-4595-eb58-7fa8dfbb7898"
      },
      "source": [
        "!pip install markovify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "Building wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py): started\n",
            "  Building wheel for markovify (setup.py): finished with status 'done'\n",
            "  Created wheel for markovify: filename=markovify-0.9.3-py3-none-any.whl size=18602 sha256=4466bfbe526c37e32ace019a522a7d811f6e2458828610291b45ccd37a5a1bc0\n",
            "  Stored in directory: c:\\users\\francesco.farinola\\appdata\\local\\pip\\cache\\wheels\\6d\\91\\54\\a72eab4bae3af86df9a3a7588e2498bf7813f5b3d06813a3e7\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.3 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyVBzAPcVUWn"
      },
      "source": [
        "import markovify\n",
        "corpus = [text for text in df.Text]\n",
        "corpus = reduce(lambda x,y: x+y, corpus)\n",
        "model = markovify.NewlineText(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNRnS7RKWPZD",
        "outputId": "8212a5cf-8c72-40ea-d7a5-24af358cdad4"
      },
      "source": [
        "for i in range(2):\n",
        "    print()\n",
        "    for i in range(0, 3):\n",
        "        print(model.make_short_sentence(40))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "e li atti loro a veder care,\n",
            "de l’Eneïda dico, la qual mi fece dono.\n",
            "distruggitor di sé claustro\n",
            "\n",
            "con Amiclate, al suon del trino spiro,\n",
            "che mi dà di pianger più che burro.\n",
            "che quel de l’Arca,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "_-QFy9YdlSdg",
        "outputId": "19f0840f-cf50-4456-ec08-e312b166ea6e"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def plotWordFrequency(df):\n",
        "    words = [line.split() for text in df.Text for line in text.split(\"\\n\") ]\n",
        "    words = reduce(lambda x,y: x+y, words)\n",
        "    data = sorted([(w, words.count(w)) for w in set(words)], key = lambda x:x[1], reverse=True)[:40] \n",
        "    most_words = [x[0] for x in data]\n",
        "    times_used = [int(x[1]) for x in data]\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.bar(x=sorted(most_words), height=times_used, color = 'grey', edgecolor = 'black',  width=.5)\n",
        "    plt.xticks(rotation=45, fontsize=18)\n",
        "    plt.yticks(rotation=0, fontsize=18)\n",
        "    plt.xlabel('Most Common Words:', fontsize=18)\n",
        "    plt.ylabel('Number of Occurences:', fontsize=18)\n",
        "    plt.show()\n",
        "\n",
        "plotWordFrequency(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CImegTNMrxtf"
      },
      "source": [
        "# Character-level text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8wNgy1WM8Ln"
      },
      "source": [
        "We lower the text as character-level text generation works better w/o capital letters. Without lowering, the model would have double the classes as output and this may result in more memory usage and maybe also misleading predictions.\n",
        "\n",
        "We make a copy of the dataset before preprocessing, since we are going to use the same dataset later for text generation using seq2seq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eFEP9QClkAn"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6U2L0eb-lkz",
        "outputId": "b529cf80-8f13-494a-a10f-cf1a4a30833f"
      },
      "source": [
        "PREPROCESSING_PIPELINE = [lower]\n",
        "df1 = df.copy()\n",
        "df1['Text'] = df1['Text'].apply(lambda x: preprocessing(x))\n",
        "df1['Text'][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"nel mezzo del cammin di nostra vita \\n mi ritrovai per una selva oscura \\n chè la diritta via era smarrita \\n ahi quanto a dir qual era è cosa dura \\n esta selva selvaggia e aspra e forte \\n che nel pensier rinova la paura \\n tant' è amara che poco è più morte \\n ma per trattar del ben ch'i' vi trovai \\n dirò de l'altre cose ch'i' v'ho scorte \\n io non so ben ridir com' i' v'intrai \\n tant' era pien di sonno a quel punto \\n che la verace via abbandonai \\n ma poi ch'i' fui al piè d'un colle giunto \\n là dove terminava quella valle \\n che m'avea di paura il cor compunto \\n guardai in alto e vidi le sue spalle \\n vestite già de' raggi del pianeta \\n che mena dritto altrui per ogne calle \\n allor fu la paura un poco queta \\n che nel lago del cor m'era durata \\n la notte ch'i' passai con tanta pieta \\n e come quei che con lena affannata \\n uscito fuor del pelago a la riva \\n si volge a l'acqua perigliosa e guata \\n così l'animo mio ch'ancor fuggiva \\n si volse a retro a rimirar lo passo \\n che non lasciò già mai persona viva \\n poi ch'èi posato un poco il corpo lasso \\n ripresi via per la piaggia diserta \\n sì che 'l piè fermo sempre era 'l più basso \\n ed ecco quasi al cominciar de l'erta \\n una lonza leggera e presta molto \\n che di pel macolato era coverta \\n e non mi si partia dinanzi al volto \\n anzi 'mpediva tanto il mio cammino \\n ch'i' fui per ritornar più volte vòlto \\n temp' era dal principio del mattino \\n e 'l sol montava 'n sù con quelle stelle \\n ch'eran con lui quando l'amor divino \\n mosse di prima quelle cose belle \\n sì ch'a bene sperar m'era cagione \\n di quella fiera a la gaetta pelle \\n l'ora del tempo e la dolce stagione \\n ma non sì che paura non mi desse \\n la vista che m'apparve d'un leone \\n questi parea che contra me venisse \\n con la test' alta e con rabbiosa fame \\n sì che parea che l'aere ne tremesse \\n ed una lupa che di tutte brame \\n sembiava carca ne la sua magrezza \\n e molte genti fè già viver grame \\n questa mi porse tanto di gravezza \\n con la paura ch'uscia di sua vista \\n ch'io perdei la speranza de l'altezza \\n e qual è quei che volontieri acquista \\n e giugne 'l tempo che perder lo face \\n che 'n tutti suoi pensier piange e s'attrista \\n tal mi fece la bestia sanza pace \\n che venendomi 'ncontro a poco a poco \\n mi ripigneva là dove 'l sol tace \\n mentre ch'i' rovinava in basso loco \\n dinanzi a li occhi mi si fu offerto \\n chi per lungo silenzio parea fioco \\n quando vidi costui nel gran diserto \\n miserere di me gridai a lui \\n qual che tu sii od ombra od omo certo \\n rispuosemi non omo omo già fui \\n e li parenti miei furon lombardi \\n mantoani per patria ambedui \\n nacqui sub iulio ancor che fosse tardi \\n e vissi a roma sotto 'l buono augusto \\n nel tempo de li dèi falsi e bugiardi \\n poeta fui e cantai di quel giusto \\n figliuol d'anchise che venne di troia \\n poi che 'l superbo iliòn fu combusto \\n ma tu perchè ritorni a tanta noia \\n perchè non sali il dilettoso monte \\n ch'è principio e cagion di tutta gioia \\n or se' tu quel virgilio e quella fonte \\n che spandi di parlar sì largo fiume \\n rispuos' io lui con vergognosa fronte \\n o de li altri poeti onore e lume \\n vagliami 'l lungo studio e 'l grande amore \\n che m'ha fatto cercar lo tuo volume \\n tu se' lo mio maestro e 'l mio autore \\n tu se' solo colui da cu' io tolsi \\n lo bello stilo che m'ha fatto onore \\n vedi la bestia per cu' io mi volsi \\n aiutami da lei famoso saggio \\n ch'ella mi fa tremar le vene e i polsi \\n a te convien tenere altro viaggio \\n rispuose poi che lagrimar mi vide \\n se vuo' campar d'esto loco selvaggio \\n chè questa bestia per la qual tu gride \\n non lascia altrui passar per la sua via \\n ma tanto lo 'mpedisce che l'uccide \\n e ha natura sì malvagia e ria \\n che mai non empie la bramosa voglia \\n e dopo 'l pasto ha più fame che pria \\n molti son li animali a cui s'ammoglia \\n e più saranno ancora infin che 'l veltro \\n verrà che la farà morir con doglia \\n questi non ciberà terra nè peltro \\n ma sapienza amore e virtute \\n e sua nazion sarà tra feltro e feltro \\n di quella umile italia fia salute \\n per cui morì la vergine cammilla \\n eurialo e turno e niso di ferute \\n questi la caccerà per ogne villa \\n fin che l'avrà rimessa ne lo 'nferno \\n là onde 'nvidia prima dipartilla \\n ond' io per lo tuo me' penso e discerno \\n che tu mi segui e io sarò tua guida \\n e trarrotti di qui per loco etterno \\n ove udirai le disperate strida \\n vedrai li antichi spiriti dolenti \\n ch'a la seconda morte ciascun grida \\n e vederai color che son contenti \\n nel foco perchè speran di venire \\n quando che sia a le beate genti \\n a le quai poi se tu vorrai salire \\n anima fia a ciò più di me degna \\n con lei ti lascerò nel mio partire \\n chè quello imperador che là sù regna \\n perch' i' fu' ribellante a la sua legge \\n non vuol che 'n sua città per me si vegna \\n in tutte parti impera e quivi regge \\n quivi è la sua città e l'alto seggio \\n oh felice colui cu' ivi elegge \\n e io a lui poeta io ti richeggio \\n per quello dio che tu non conoscesti \\n acciò ch'io fugga questo male e peggio \\n che tu mi meni là dov' or dicesti \\n sì ch'io veggia la porta di san pietro \\n e color cui tu fai cotanto mesti \\n allor si mosse e io li tenni dietro \\n \""
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI5nzsjtNThT"
      },
      "source": [
        "First, we flatten the text corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUan-b40QtSW"
      },
      "source": [
        "corpus = [text for text in df1.Text]\n",
        "corpus = reduce(lambda x,y: x+y, corpus)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghWXWjj3NXgC"
      },
      "source": [
        "Then, we create the character listing along with the relative dictioneries that map each character to an integer and viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1btv4bQMKfmD"
      },
      "source": [
        "def create_idx(df):\n",
        "  unique_chars = set()\n",
        "  for text in df.Text:\n",
        "    unique_chars = list(set(unique_chars) | set(text))\n",
        "  unique_chars.sort()\n",
        "  char2idx = {char[1]: char[0] for char in enumerate(unique_chars)}\n",
        "  idx2char = {v: k for k, v in char2idx.items()}\n",
        "  return unique_chars, char2idx, idx2char\n",
        "\n",
        "char_listing, char2idx, idx2char = create_idx(df1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7kgwvTbkjxb",
        "outputId": "800ce682-e813-429b-e0a7-8d9cc8250784"
      },
      "source": [
        "print(char2idx)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '\"': 2, \"'\": 3, '-': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'j': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'x': 26, 'y': 27, 'z': 28, 'à': 29, 'è': 30, 'ì': 31, 'ò': 32, 'ù': 33, '‘': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AsLAMhNkwE"
      },
      "source": [
        "We transform the corpus text into an encoded corpus where each integer correspond to a character using the previously defined dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjkQeyx-URr9"
      },
      "source": [
        "def numerical_encoding(df, char2idx):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [char for text in df.Text for char in text]\n",
        "    chars_list = [char2idx[char] for char in chars_list]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n",
        "\n",
        "def decode_sequence(seq):\n",
        "  decoded = [idx2char[i] for i in seq]\n",
        "  return ''.join(decoded)\n",
        "\n",
        "encoded_corpus = numerical_encoding(df1, char2idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLy1AIHplqUb"
      },
      "source": [
        "##Input preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPCngnocOnmm"
      },
      "source": [
        "In the following cell:\n",
        "\n",
        "\n",
        "*   `from_tensor_slices`: creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. \n",
        "\n",
        "  Then apply `batch` since we have a sequence of integers representing characters which is much longer so it is much helpful if we divide this into many sequences of `SEQUENCE_LENGTH+1`.\n",
        "  \n",
        "  We use `drop_remainder=True` whether the last batch should be dropped in the case it has fewer than batch_size elements;\n",
        "\n",
        "*   Creating batches pipeline:\n",
        "  \n",
        "  1. `map` for each sequence created before with `from_tensor_slices` applies the function `split_input_target` which returns the input and output sequences for the model. i.e. We have a sequence `Nel mezzo del`: respectively the input text will be `Nel mezzo de` and the output will be `el mezzo del`\n",
        "  2. `shuffle` namely shuffles the sequences\n",
        "  3. `batch` namely creates batches of length `BATCH_SIZE` with sequences shuffled.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LShR3sAWGXq0",
        "outputId": "89d2bd88-2ff1-4972-a670-5263b3292435"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 10000\n",
        "example_per_epoch = len(corpus)//SEQ_LENGTH\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "sequences = tf.data.Dataset.from_tensor_slices(encoded_corpus).batch(batch_size=SEQ_LENGTH+1, drop_remainder=True)\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_ex, target_ex in dataset.take(2):\n",
        "  print('Input data: ', repr(decode_sequence(input_ex.numpy())))\n",
        "  print('Output data: ', repr(decode_sequence(target_ex.numpy())))\n",
        "  print(\"\\n\")\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'nel mezzo del cammin di nostra vita \\n mi ritrovai per una selva oscura \\n chè la diritta via era smar'\n",
            "Output data:  'el mezzo del cammin di nostra vita \\n mi ritrovai per una selva oscura \\n chè la diritta via era smarr'\n",
            "\n",
            "\n",
            "Input data:  'ita \\n ahi quanto a dir qual era è cosa dura \\n esta selva selvaggia e aspra e forte \\n che nel pensier'\n",
            "Output data:  'ta \\n ahi quanto a dir qual era è cosa dura \\n esta selva selvaggia e aspra e forte \\n che nel pensier '\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK5q23cMlu_-"
      },
      "source": [
        "##Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kte9niUhJHs-",
        "outputId": "c561a0a6-1a84-4ffa-a452-7c4442f528e5"
      },
      "source": [
        "VOCAB_SIZE = len(char_listing)# The embedding dimension\n",
        "EMBEDDING_DIM = 256 \n",
        "UNITS = 512\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim = embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "        tf.keras.layers.GRU(units * 2, return_sequences= True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.GRU(units, return_sequences= True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size = VOCAB_SIZE,\n",
        "                    embedding_dim = EMBEDDING_DIM,\n",
        "                    units = UNITS,\n",
        "                    batch_size = BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 256)          8960      \n",
            "                                                                 \n",
            " gru (GRU)                   (128, None, 1024)         3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (128, None, 512)          2362368   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 35)           17955     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,327,587\n",
            "Trainable params: 6,327,587\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0tS6HGSKPex",
        "outputId": "2d21597e-2bc1-45c4-87bb-64528433a28e"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='Adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "EPOCHS = 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, early_stop])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - 20s 186ms/step - loss: 2.9861\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 8s 186ms/step - loss: 2.2960\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 8s 188ms/step - loss: 2.0390\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 8s 187ms/step - loss: 1.8831\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 8s 188ms/step - loss: 1.7802\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 8s 188ms/step - loss: 1.7023\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 8s 188ms/step - loss: 1.6326\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 8s 190ms/step - loss: 1.5733\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 8s 190ms/step - loss: 1.5223\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 8s 190ms/step - loss: 1.4791\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 8s 191ms/step - loss: 1.4391\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 8s 191ms/step - loss: 1.4065\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 8s 191ms/step - loss: 1.3739\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 8s 191ms/step - loss: 1.3457\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 8s 193ms/step - loss: 1.3184\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 8s 192ms/step - loss: 1.2896\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 8s 192ms/step - loss: 1.2619\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 8s 193ms/step - loss: 1.2322\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 8s 192ms/step - loss: 1.2047\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 9s 193ms/step - loss: 1.1736\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 9s 193ms/step - loss: 1.1429\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 8s 194ms/step - loss: 1.1068\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 9s 194ms/step - loss: 1.0711\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 9s 194ms/step - loss: 1.0320\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 9s 194ms/step - loss: 0.9910\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.9496\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.9052\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.8576\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.8089\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 8s 194ms/step - loss: 0.7607\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.7125\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.6686\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 9s 194ms/step - loss: 0.6223\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 8s 195ms/step - loss: 0.5802\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 8s 195ms/step - loss: 0.5405\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 8s 195ms/step - loss: 0.5037\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.4731\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.4423\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 8s 195ms/step - loss: 0.4175\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.3957\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.3794\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.3610\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.3450\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.3363\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 9s 195ms/step - loss: 0.3266\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.3164\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.3086\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.3021\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.2962\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 9s 196ms/step - loss: 0.2898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlmaVDResQQD"
      },
      "source": [
        "##Temperature Char-level Sampling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xyJWJaPtq-d"
      },
      "source": [
        "We first create a new inference model with `batch_size = 1`, load the latest checkpoint weights and define the input shape.\n",
        "\n",
        "The function `char_level_temperature_sampling` takes in input \n",
        "* the model (`model`), \n",
        "* the start string to define the context (`start_string`), \n",
        "* the number of characters to generate (`chars_to_generate`) and \n",
        "* the `temperature` which defines how distant the predictions are far from the dataset (the smaller, the more similar to the input dataset)\n",
        "\n",
        "After encoding the input_string which defines the context, we reset the states of the model to make independent call from the fit method, and then start to generate the next characters with iterative calls to the model.\n",
        "\n",
        "Only in the first call we pass the entire input string since the model is stateful and we do not need to pass the whole sequence on recurrent calls. So as, when recurrently calling the model, we simply pass the last character ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVvT-WlhKlR9"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, UNITS, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrreJ0XCKup4",
        "outputId": "68d04a0f-756b-4dee-d4f2-67d1e0328b86"
      },
      "source": [
        "def char_level_temperature_sampling(model, start_string, chars_to_generate, temperature):\n",
        "    input_eval = [char2idx[s] for s in start_string] #Convert input_string to correspondent encoding\n",
        "    input_eval = tf.expand_dims(input_eval, axis=0) #reshape according to model input shape (1, None)\n",
        "    text_generated = [] #string for storing results \n",
        "    model.reset_states() #reset states of the model to make calls to the model independent from previous calls (fit, predict, evaluate)\n",
        "    for i in range(chars_to_generate):\n",
        "        predictions = model(input_eval) #predict the next character\n",
        "        predictions = tf.squeeze(predictions, 0) #squeeze results\n",
        "        predictions = predictions / temperature #apply temperature\n",
        "        \n",
        "        #Draws num_samples from a categorical distribution of logits\n",
        "        #This is applied to the last timestep (-1) to get the next char to generate\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() \n",
        "        \n",
        "        # Pass the predicted character as the next input to the \n",
        "        # model along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)   \n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return ''.join(text_generated)\n",
        "\n",
        "start_string = u\"nel mezzo del cammin di nostra vita mi ritrovai\"\n",
        "char_level_generation = char_level_temperature_sampling(model, start_string=start_string, \n",
        "                                                        chars_to_generate=500, temperature=0.5)\n",
        "print(start_string, char_level_generation)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nel mezzo del cammin di nostra vita mi ritrovai  \n",
            " tenean la testa e ancor tutto 'l casso \n",
            " e di costoro assai riconobb' io \n",
            " con più color che sono in terra \n",
            " tutti sviati dietro al malo essemplo \n",
            " già si solea con le spade farne \n",
            " sorte pria che sia la corda queta \n",
            " così corremmo nel vocabo prima \n",
            " che le più alte cime più percuote \n",
            " e ciò non fa d'onor poco argomento \n",
            " per che se tu a la virtù che vole \n",
            " freno a dimandar più tardo \n",
            " intra due cibi distanti e moventi \n",
            " d'un modo e l'altro insieme \n",
            " l'un de l'altro giacea e qual carpone \n",
            " si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQhKkbcwtMbI"
      },
      "source": [
        "# Seq2Seq text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwL816bb4hHS"
      },
      "source": [
        "In this section we will experiment other text generation techniques using a Seq2Seq model. We will use:\n",
        "\n",
        "* SpaCy for Italian word embeddings\n",
        "\n",
        "* SymSpellPy italian vocabulary to handle out-of-vocabulary words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKtKSanypZuP"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy --upgrade\n",
        "!python -m spacy download it_core_news_lg\n",
        "import spacy\n",
        "nlp = spacy.load(\"it_core_news_lg\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf2Pkftq5JBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a686303d-df5c-4d2b-ef13-e99b18804319"
      },
      "source": [
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path=path+\"it-100k.txt\"\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in c:\\users\\francesco.farinola\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (6.7.5)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in c:\\users\\francesco.farinola\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from symspellpy) (0.1.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCcQKbZFlB02"
      },
      "source": [
        "Adjust the newline token to get a better splitting on sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OqBweO7lA46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7029b3da-8493-4c86-f75d-8366b2f2a598"
      },
      "source": [
        "PREPROCESSING_PIPELINE = [adjust_newline]\n",
        "df['Text'] = df['Text'].apply(lambda x: preprocessing(x))\n",
        "df['Text'][0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nel mezzo del cammin di nostra vita \\nmi ritrovai per una selva oscura \\nchè la diritta via era smarrita \\nAhi quanto a dir qual era è cosa dura \\nesta selva selvaggia e aspra e forte \\nche nel pensier rinova la paura \\nTant' è amara che poco è più morte \\nma per trattar del ben ch'i' vi trovai \\ndirò de l'altre cose ch'i' v'ho scorte \\nIo non so ben ridir com' i' v'intrai \\ntant' era pien di sonno a quel punto \\nche la verace via abbandonai \\nMa poi ch'i' fui al piè d'un colle giunto \\nlà dove terminava quella valle \\nche m'avea di paura il cor compunto \\nguardai in alto e vidi le sue spalle \\nvestite già de' raggi del pianeta \\nche mena dritto altrui per ogne calle \\nAllor fu la paura un poco queta \\nche nel lago del cor m'era durata \\nla notte ch'i' passai con tanta pieta \\nE come quei che con lena affannata \\nuscito fuor del pelago a la riva \\nsi volge a l'acqua perigliosa e guata \\ncosì l'animo mio ch'ancor fuggiva \\nsi volse a retro a rimirar lo passo \\nche non lasciò già mai persona viva \\nPoi ch'èi posato un poco il corpo lasso \\nripresi via per la piaggia diserta \\nsì che 'l piè fermo sempre era 'l più basso \\nEd ecco quasi al cominciar de l'erta \\nuna lonza leggera e presta molto \\nche di pel macolato era coverta \\ne non mi si partia dinanzi al volto \\nanzi 'mpediva tanto il mio cammino \\nch'i' fui per ritornar più volte vòlto \\nTemp' era dal principio del mattino \\ne 'l sol montava 'n sù con quelle stelle \\nch'eran con lui quando l'amor divino \\nmosse di prima quelle cose belle \\nsì ch'a bene sperar m'era cagione \\ndi quella fiera a la gaetta pelle \\nl'ora del tempo e la dolce stagione \\nma non sì che paura non mi desse \\nla vista che m'apparve d'un leone \\nQuesti parea che contra me venisse \\ncon la test' alta e con rabbiosa fame \\nsì che parea che l'aere ne tremesse \\nEd una lupa che di tutte brame \\nsembiava carca ne la sua magrezza \\ne molte genti fè già viver grame \\nquesta mi porse tanto di gravezza \\ncon la paura ch'uscia di sua vista \\nch'io perdei la speranza de l'altezza \\nE qual è quei che volontieri acquista \\ne giugne 'l tempo che perder lo face \\nche 'n tutti suoi pensier piange e s'attrista \\ntal mi fece la bestia sanza pace \\nche venendomi 'ncontro a poco a poco \\nmi ripigneva là dove 'l sol tace \\nMentre ch'i' rovinava in basso loco \\ndinanzi a li occhi mi si fu offerto \\nchi per lungo silenzio parea fioco \\nQuando vidi costui nel gran diserto \\nMiserere di me gridai a lui \\nqual che tu sii od ombra od omo certo \\nRispuosemi Non omo omo già fui \\ne li parenti miei furon lombardi \\nmantoani per patria ambedui \\nNacqui sub Iulio ancor che fosse tardi \\ne vissi a Roma sotto 'l buono Augusto \\nnel tempo de li dèi falsi e bugiardi \\nPoeta fui e cantai di quel giusto \\nfigliuol d'Anchise che venne di Troia \\npoi che 'l superbo Iliòn fu combusto \\nMa tu perchè ritorni a tanta noia \\nperchè non sali il dilettoso monte \\nch'è principio e cagion di tutta gioia \\nOr se' tu quel Virgilio e quella fonte \\nche spandi di parlar sì largo fiume \\nrispuos' io lui con vergognosa fronte \\nO de li altri poeti onore e lume \\nvagliami 'l lungo studio e 'l grande amore \\nche m'ha fatto cercar lo tuo volume \\nTu se' lo mio maestro e 'l mio autore \\ntu se' solo colui da cu' io tolsi \\nlo bello stilo che m'ha fatto onore \\nVedi la bestia per cu' io mi volsi \\naiutami da lei famoso saggio \\nch'ella mi fa tremar le vene e i polsi \\nA te convien tenere altro viaggio \\nrispuose poi che lagrimar mi vide \\nse vuo' campar d'esto loco selvaggio \\nchè questa bestia per la qual tu gride \\nnon lascia altrui passar per la sua via \\nma tanto lo 'mpedisce che l'uccide \\ne ha natura sì malvagia e ria \\nche mai non empie la bramosa voglia \\ne dopo 'l pasto ha più fame che pria \\nMolti son li animali a cui s'ammoglia \\ne più saranno ancora infin che 'l veltro \\nverrà che la farà morir con doglia \\nQuesti non ciberà terra nè peltro \\nma sapienza amore e virtute \\ne sua nazion sarà tra feltro e feltro \\nDi quella umile Italia fia salute \\nper cui morì la vergine Cammilla \\nEurialo e Turno e Niso di ferute \\nQuesti la caccerà per ogne villa \\nfin che l'avrà rimessa ne lo 'nferno \\nlà onde 'nvidia prima dipartilla \\nOnd' io per lo tuo me' penso e discerno \\nche tu mi segui e io sarò tua guida \\ne trarrotti di qui per loco etterno \\nove udirai le disperate strida \\nvedrai li antichi spiriti dolenti \\nch'a la seconda morte ciascun grida \\ne vederai color che son contenti \\nnel foco perchè speran di venire \\nquando che sia a le beate genti \\nA le quai poi se tu vorrai salire \\nanima fia a ciò più di me degna \\ncon lei ti lascerò nel mio partire \\nchè quello imperador che là sù regna \\nperch' i' fu' ribellante a la sua legge \\nnon vuol che 'n sua città per me si vegna \\nIn tutte parti impera e quivi regge \\nquivi è la sua città e l'alto seggio \\noh felice colui cu' ivi elegge \\nE io a lui Poeta io ti richeggio \\nper quello Dio che tu non conoscesti \\nacciò ch'io fugga questo male e peggio \\nche tu mi meni là dov' or dicesti \\nsì ch'io veggia la porta di san Pietro \\ne color cui tu fai cotanto mesti \\nAllor si mosse e io li tenni dietro \\n\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60F-zaM3Mj2K"
      },
      "source": [
        "corpus = [text for text in df.Text]\n",
        "corpus = reduce(lambda x,y: x+y, corpus)\n",
        "tokens = nlp(corpus)\n",
        "sequences = [nlp(line) for text in df.Text for line in text.split(\" \\n\")]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zALkCSZ76wuy"
      },
      "source": [
        "##Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S62Rqy4S7HIB"
      },
      "source": [
        "The `spell_correction` function looks up for the most similar word using the symspellpy italian vocabulary with `max_edit_distance = 1`.\n",
        "\n",
        "This function will be called only if the strategy of the `compute_embeddings` function will be `similarity`. By doing so, when assigning word embeddings of OOV words, this function will try to spell correctly a word in order to assign the word embedding of the word spelled correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6gr9EQEiZQR"
      },
      "source": [
        "def spell_correction(text):\n",
        "    results = [t if (t.isnumeric() or  len(t)<=3)\n",
        "               else sym_spell.lookup(t, Verbosity.TOP, max_edit_distance=1, \n",
        "                                     include_unknown=True)[0].term \n",
        "               for t in text.split()]\n",
        "    return ' '.join(results)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eASVPeHa8ep8"
      },
      "source": [
        "The `compute_embeddings` function assigns SpaCy word embeddings to in-vocabulary words and handle OOV terms in different ways:\n",
        "\n",
        "* `strategy='similarity'` and `random_oov=False`: will spell correct the oov term and try to assign the embedding of the similar word\n",
        "\n",
        "* `strategy='similarity'` and `random_oov=False`: will assign to the oov term a random vector\n",
        "\n",
        "* `strategy='random'`: will assign to all the word embeddings a random vector, including in-vocabulary terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKvR7R4Y5zqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b97c1b-6c50-4f91-b591-07948fdabc09"
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "def compute_embeddings(tokens, nlp, EMBEDDING_DIM, strategy=\"similarity\", random_oov=False):\n",
        "    print(f\"There are {len(list(set([word.text for word in tokens])))} unique tokens\")\n",
        "    embeddings = {} #Initialize embedding dict\n",
        "    oov_words = {} #Initialize oov dict\n",
        "    cc = 0 #counter for oov terms\n",
        "\n",
        "    #Initializing special words embeddings\n",
        "    embeddings[\"<PAD>\"] = np.zeros((EMBEDDING_DIM,))\n",
        "    embeddings[\"<SOS>\"] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "    embeddings[\"<EOS>\"] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "    embeddings[\"\\n \"] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "    \n",
        "    for word in tokens: #for each word in the dataset\n",
        "      if word.text not in embeddings: #if a unique word has not been processed yet\n",
        "        if strategy == \"similarity\":\n",
        "          if word.has_vector: #if the word has a corresponding word vector in SpaCy\n",
        "            embeddings[word.text] = word.vector #Add the word vector to the vocabulary\n",
        "          elif random_oov: #If random strategy for oov terms assign a random vector\n",
        "            cc = cc+1\n",
        "            embeddings[word.text] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "          else:\n",
        "            cc = cc+1\n",
        "            similar_word = spell_correction(word.text) #Find a similar word\n",
        "            similar_token = nlp(similar_word) #Get the corresponding token in SpaCy\n",
        "            oov_words[word.text] = similar_word #Fill the oov dict with [misspelled, correclty spelled] word\n",
        "            #If the new word has a vector we assign the vector to the original word, otherwise a random vector\n",
        "            if similar_token.has_vector and similar_word != word.text: \n",
        "                embeddings[word.text] = similar_token.vector\n",
        "            else:\n",
        "                embeddings[word.text] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "        #If random strategy assign to all words a random vector\n",
        "        if strategy == \"random\":\n",
        "          embeddings[word.text] = np.random.uniform(-1, 1, (EMBEDDING_DIM,))\n",
        "\n",
        "\n",
        "    print(f\"There are {cc} words for which we created an embedding\")\n",
        "    #Compute idx2word and word2idx dictionaries\n",
        "    idx2word = dict([(idx,v) for idx,v in enumerate(list(embeddings.keys()))])\n",
        "    word2idx = {v: k for k, v in idx2word.items()}\n",
        "\n",
        "    #Initialize  the embedding matrix as a numpy array and fill it with all the vectors throught the embeddings dict\n",
        "    embedding_matrix = np.zeros((len(idx2word), EMBEDDING_DIM))\n",
        "    for word, i in word2idx.items():\n",
        "        embedding_vector = embeddings[word]\n",
        "        #Words not found in embedding index will be all-zeros.\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix, oov_words, idx2word, word2idx\n",
        "\n",
        "\n",
        "embedding_matrix, oov_dict, idx2word, word2idx = compute_embeddings(tokens, nlp, EMBEDDING_DIM)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 13529 unique tokens\n",
            "There are 4059 words for which we created an embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsRbmMAOtfA"
      },
      "source": [
        "Here, we check how some OOV are handled, with which term and vector they are substituted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocZdG6JT-hyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7af19e-49de-4f10-cf5a-5890af27832e"
      },
      "source": [
        "print(\"Some out-of-vocabulary words with their respective word substituted:\")\n",
        "for k, v in list(oov_dict.items())[2:10]:\n",
        "    oovtest = nlp(v)\n",
        "    print(f\"OOV word: {k}, \\t vector used: {v} \\t {oovtest.vector[:5]}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "zero_vectors = np.where(np.count_nonzero(embedding_matrix, axis=1)==0)[0]\n",
        "print(f\"There are {zero_vectors.shape[0]} words with zero vectors. Here we show some:\")\n",
        "print(np.array([idx2word[i] for i in zero_vectors])[:50])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some out-of-vocabulary words with their respective word substituted:\n",
            "OOV word: ridir, \t vector used: ridir \t [0. 0. 0. 0. 0.]\n",
            "OOV word: intrai, \t vector used: entrai \t [ 0.83551  0.2146  -0.78065 -1.2835  -1.451  ]\n",
            "OOV word: èi, \t vector used: èi \t [0. 0. 0. 0. 0.]\n",
            "OOV word: macolato, \t vector used: maculato \t [-0.38396  0.85145 -0.33377 -1.6229  -1.5451 ]\n",
            "OOV word: mpediva, \t vector used: impediva \t [ 0.76814  1.9516  -1.61    -0.35673 -0.10835]\n",
            "OOV word: gaetta, \t vector used: gaeta \t [ 0.88015  -1.9323    1.1862    0.2662   -0.023887]\n",
            "OOV word: tremesse, \t vector used: premesse \t [ 1.6344    0.78347  -0.039451  1.1204    1.7222  ]\n",
            "OOV word: sembiava, \t vector used: sembrava \t [ 1.3384   1.3271  -0.76626 -1.1606  -0.10625]\n",
            "\n",
            "\n",
            "There are 97 words with zero vectors. Here we show some:\n",
            "['<PAD>' '\\n' 'risonavan' 'gittansi' 'ubidente' 'salutevol' 'farmisi'\n",
            " 'mugghia' 'disiato' 'caggiono' 'rabbuffa' 'avaccio' 'ruffian' 'adeschi'\n",
            " 'menommi' 'tacerci' 'dienno' 'distorse' 'appressavan' 'temesti'\n",
            " 'parlasia' 'navicar' 'disfaccia' 'Danar' 'Isopo' 'venieno' 'aggroppate'\n",
            " 'biece' 'Ogne' 'nvidio' 'scorgessi' 'guerir' 'triunfar' 'ossame'\n",
            " 'discarno' 'pontan' 'zebe' 'Beccheria' 'impetrai' 'sovvegna' 'ristrinsi'\n",
            " 'affisar' 'Oriaco' 'sodisfar' 'inforcar' 'ncarco' 'sodisfaccia'\n",
            " 'superbite' 'secondamente' 'iracundia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIdgTbySixF6"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ab1krNi_-P"
      },
      "source": [
        "We inspect the sequences length to define a proper maximum sequence length for the model.\n",
        "We will set two maximum sequence lengths: one for the encoder and one for the decoder. The decoder will have 1 token more since its input will include the `<SOS>` token and the output the `<EOS>` token.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T31jP_TTCa3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f09a10-9d58-4fe3-c07b-705104354561"
      },
      "source": [
        "def check_max_sequence_length(sequences):\n",
        "    from collections import Counter\n",
        "    len_seq = [len(s) for s in sequences]\n",
        "    len_frequencies = Counter(list(len_seq))\n",
        "    print(f\"List of tuples of (line length, # of lines):\\n{sorted(len_frequencies.items())}\")\n",
        "\n",
        "    #Print unnecessary lines which are too short or too long that may cause misleading predictions\n",
        "    #Also choosing a bigger MAX_SEQUENCE_LENGTH may cause more computational time.\n",
        "    print(f\"There are {len_frequencies[1]} lines with length 0 (will be excluded from dataset)\")\n",
        "    for i, value in enumerate(len_seq):\n",
        "      if value==3 or value>=14:\n",
        "        print(f\"Length: {value}\\t Line: {sequences[i]}\")\n",
        "\n",
        "check_max_sequence_length(sequences)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of tuples of (line length, # of lines):\n",
            "[(0, 100), (3, 12), (4, 182), (5, 1046), (6, 2802), (7, 3991), (8, 3351), (9, 1786), (10, 722), (11, 252), (12, 63), (13, 20), (14, 4), (15, 2)]\n",
            "There are 0 lines with length 0 (will be excluded from dataset)\n",
            "Length: 15\t Line: l'umana spezie e 'l loco e 'l tempo e 'l seme\n",
            "Length: 14\t Line: e sta 'n su quel più che 'n su l'altro eretto\n",
            "Length: 14\t Line: così com' ella sie' tra 'l piano e 'l monte\n",
            "Length: 14\t Line: tra 'l quinto dì e 'l sesto ond' io mi diedi\n",
            "Length: 3\t Line: maravigliando diventaro smorte\n",
            "Length: 15\t Line: tra 'l Po e 'l monte e la marina e 'l Reno\n",
            "Length: 3\t Line: lungamente mostrando paganesmo\n",
            "Length: 3\t Line: apparecchiava grazioso loco\n",
            "Length: 3\t Line: superillustrans claritate tua\n",
            "Length: 3\t Line: silogizzò invidiosi veri\n",
            "Length: 3\t Line: cotanto gloriosamente accolto\n",
            "Length: 3\t Line: etternalmente rimanendosi una\n",
            "Length: 3\t Line: DILIGITE IUSTITIAM primai\n",
            "Length: 3\t Line: surgono innumerabili faville\n",
            "Length: 3\t Line: così benedicendomi cantando\n",
            "Length: 14\t Line: di' quel ch'ell' è di' come se ne 'nfiora\n",
            "Length: 3\t Line: perpetualemente Osanna sberna\n",
            "Length: 3\t Line: stupefaciensi quando Laterano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGOFulise4U"
      },
      "source": [
        "We choose a `MAX_SEQ_LEN` of 11 as a tradeoff in order to discard few and unnecessary long sequences that would increase the model complexity and training time.\n",
        "\n",
        "Then, we clean the dataset from too long and too short sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTR_T9a7QVw8"
      },
      "source": [
        "MAX_SEQ_LEN_ENCODER = 11\n",
        "MAX_SEQ_LEN_DECODER = MAX_SEQ_LEN_ENCODER + 1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvPqTjT1UXQG"
      },
      "source": [
        "def clean_dataset(sequences, MAX_SEQ_LEN_ENCODER):\n",
        "    len_seq = [len(s) for s in sequences]\n",
        "    indices = []\n",
        "    #Loop to get hte indexes of too long and short sequences\n",
        "    for i, value in enumerate(len_seq):\n",
        "        #Select sequences of lengtg of 3 or lesser and 11 or lesser\n",
        "        if value < 4 or value>MAX_SEQ_LEN_ENCODER:\n",
        "            indices.append(i)\n",
        "    #Loop to delete sequences of found indexes\n",
        "    for i in sorted(indices, reverse=True):\n",
        "        del sequences[i]\n",
        "    return sequences\n",
        "\n",
        "sequences = clean_dataset(sequences, MAX_SEQ_LEN_ENCODER)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzJd0FArsYfC"
      },
      "source": [
        "##Encoding model inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y2JsDUDtedv"
      },
      "source": [
        "The `encode_dataset` function transforms the sequences of the dataset into encoded ones using the word2idx dictionary.\n",
        "\n",
        "In an Encoder/Decoder model designed for text generation we have two parts which receive and produce different inputs and outputs respectively.\n",
        "\n",
        "Assuming the special tokens `<PAD> = 0`, `<SOS> = 1`, `<EOS> = 2`\n",
        "\n",
        "The Encoder has the task to produce a hidden representation of the sentence: so, it will receive as input the encoded sentence padded without any additional token (i.e. sentence to be fed = \"Nel mezzo del cammin di nostra vita\", input to be fed to the encoder = [3 4 5 6 7 8 9 0 0 0 0]) and produce through an RNN the states that will gives a better context representation and will be passed to the decoder.\n",
        "\n",
        "The Decoder instead will be fed with inputs and outputs:\n",
        "* the input is in the form: `<SOS>` + encoded sentence = [1 3 4 5 6 7 8 9 0 0 0 0]\n",
        "* the output is in the form: encoded sentence + `<EOS>` = [3 4 5 6 7 8 9 2 0 0 0 0]\n",
        "\n",
        "\n",
        "`preprocess_encoder_input` encodes the input of the encoder as previously said, so we simply pad the sequence.\n",
        "\n",
        "`preprocess_decoder_input` produces the decoder input and output are previously said."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XvmSBCinL2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def encode_dataset(sequences, word2idx):\n",
        "    encoded = []\n",
        "    for seq in sequences:\n",
        "      encoded_seq = []\n",
        "      for word in seq: \n",
        "        encoded_seq.append(word2idx[word.text])\n",
        "      encoded.append(encoded_seq)  \n",
        "    return encoded\n",
        "\n",
        "def preprocess_encoder_input(input, max_len, word2idx):\n",
        "    encoder_input_data = pad_sequences(input, maxlen=max_len, padding=\"post\", truncating=\"pre\")\n",
        "    return encoder_input_data\n",
        "\n",
        "def preprocess_decoder_input(input, max_len, word2idx):\n",
        "    sos_token = [word2idx[\"<SOS>\"]]\n",
        "    eos_token = [word2idx[\"<EOS>\"]]\n",
        "    decoder_input = [sos_token + line for line in input]\n",
        "    decoder_output = [line + eos_token for line in input]\n",
        "    decoder_input_data = pad_sequences(decoder_input, maxlen=max_len, padding=\"post\", truncating=\"pre\")\n",
        "    decoder_output_data = pad_sequences(decoder_output, maxlen=max_len, padding=\"post\", truncating=\"pre\")\n",
        "    return decoder_input_data, decoder_output_data\n",
        "\n",
        "encoded_sequences = encode_dataset(sequences, word2idx)\n",
        "encoder_input = preprocess_encoder_input(encoded_sequences, MAX_SEQ_LEN_ENCODER, word2idx) \n",
        "decoder_input, decoder_output = preprocess_decoder_input(encoded_sequences, MAX_SEQ_LEN_DECODER, word2idx)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQYeNYjA84V_"
      },
      "source": [
        "Simply function to check if the inputs have been encoded correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09-jOnA5UzwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502b104b-e38a-4cef-e308-2a2896f17402"
      },
      "source": [
        "def decode_sequence(sequence, idx2word):\n",
        "    s = []\n",
        "    for i in sequence:\n",
        "        s.append(idx2word[i])\n",
        "    return \" \".join(s)\n",
        "\n",
        "print(f\"Encoder input: {decode_sequence(encoder_input[0], idx2word)}\")\n",
        "print(f\"Decoder input: {decode_sequence(decoder_input[0], idx2word)}\")\n",
        "print(f\"Decoder output: {decode_sequence(decoder_output[0], idx2word)}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input: Nel mezzo del cammin di nostra vita <PAD> <PAD> <PAD> <PAD>\n",
            "Decoder input: <SOS> Nel mezzo del cammin di nostra vita <PAD> <PAD> <PAD> <PAD>\n",
            "Decoder output: Nel mezzo del cammin di nostra vita <EOS> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_4mgtEpzMKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca835d51-c374-4bc2-b545-65b14ed994b2"
      },
      "source": [
        "#Clear cache with garbage collector\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "360"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRrqg9NU9UOX"
      },
      "source": [
        "##Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjeZhgE_-LNj"
      },
      "source": [
        "In this section we build our model. We define the classes for Encoder and Decoder:\n",
        "* Encoder: after passing the input to the Embedding layer, we go through two stacked GRUs and pass the final state to the Decoder\n",
        "\n",
        "* Decoder: is made of two stacked GRUs whose states are initialized with the output states of the Encoder final layer. The final layer of the decoder is a Dense Layer with softmax function to get the categorical probabilities for each token to be the next one.\n",
        "\n",
        "\n",
        "**NB:** We cannot use Bidirectional GRU in the Decoder part since we should not know anything about the next tokens and we are predicting the probabilities of the next tokens we can work only in one direction.\n",
        "\n",
        "Remember to always return the sequences when stacking GRU layers and to pass the state to remember the context, so to not make independent calls on layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RQfoVzWlqeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "bffc4572-926f-4c7a-d122-b65359576999"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "UNITS = 500\n",
        "BATCH_SIZE = 32\n",
        "VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
        "                                               output_dim = EMBEDDING_DIM, \n",
        "                                               input_length=MAX_SEQ_LEN_ENCODER,\n",
        "                                               trainable=True, #we set trainable=True to train word embeddings during training\n",
        "                                               mask_zero=True, #to ignore padding tokens\n",
        "                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))\n",
        "        self.gru1 = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, dropout=0.2, recurrent_initializer='glorot_uniform')\n",
        "        self.gru2 = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, dropout=0.2, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        \n",
        "        \n",
        "    def call(self, x):\n",
        "        embed = self.embed(x)\n",
        "        out, h = self.gru1(embed)\n",
        "        out, h = self.gru2(out, initial_state=h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
        "                                               output_dim = EMBEDDING_DIM, \n",
        "                                               input_length=MAX_SEQ_LEN_DECODER,\n",
        "                                               trainable=True, #we set trainable=True to train word embeddings during training\n",
        "                                               mask_zero=True, #to ignore padding tokens\n",
        "                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))\n",
        "        \"\"\"\n",
        "        Tried implementing AdditiveAttention but no success\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "        self.Wc = tf.keras.layers.Dense(UNITS, activation=\"tanh\", use_bias=False)\n",
        "        \"\"\"\n",
        "        self.gru1 = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, dropout=0.2, recurrent_initializer='glorot_uniform')\n",
        "        self.gru2 = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, dropout=0.2, recurrent_initializer='glorot_uniform')\n",
        "        #Prediction layer with softmax function to get probabilities for each token in the vocabulary\n",
        "        self.fc = tf.keras.layers.Dense(VOCAB_SIZE, activation='softmax')\n",
        "        \n",
        "    def call(self, x, init_state):\n",
        "        embed = self.embed(x)\n",
        "        out, h = self.gru1(embed, initial_state = init_state)\n",
        "        out, h = self.gru2(out, initial_state = h)\n",
        "        \"\"\"\n",
        "        context_vector, attention_weights = self.attention([out, init_state], return_attention_scores=True)\n",
        "\n",
        "        # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "        #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "        context_and_rnn_output = tf.keras.layers.Concatenate(axis=-1)([context_vector, out])\n",
        "\n",
        "        # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "        attention_vector = self.Wc(context_and_rnn_output)\n",
        "        \n",
        "        out = self.fc(attention_vector)\n",
        "        \"\"\"\n",
        "        out = self.fc(out)\n",
        "        return out, h\n",
        "\n",
        "#Initialize the Encoder and Decoder\n",
        "encoder_model = Encoder()\n",
        "decoder_model = Decoder()\n",
        "\n",
        "#Define Input shapes\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(MAX_SEQ_LEN_ENCODER,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(MAX_SEQ_LEN_DECODER,))\n",
        "\n",
        "#Assign variables to models\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "#Define the Model\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#Plot model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(seq2seq, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jU7KYVNEc5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d019a361-3543-43bd-ad37-fb49259fec85"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "seq2seq.compile(optimizer=\"nadam\", loss=loss, metrics=['accuracy'])\n",
        "seq2seq.fit([encoder_input, decoder_input], decoder_output,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "442/442 [==============================] - 29s 39ms/step - loss: 3.4694 - accuracy: 0.3446\n",
            "Epoch 2/15\n",
            "442/442 [==============================] - 17s 39ms/step - loss: 2.1447 - accuracy: 0.5316\n",
            "Epoch 3/15\n",
            "442/442 [==============================] - 18s 40ms/step - loss: 1.4156 - accuracy: 0.6324\n",
            "Epoch 4/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.8546 - accuracy: 0.7644\n",
            "Epoch 5/15\n",
            "442/442 [==============================] - 20s 45ms/step - loss: 0.4809 - accuracy: 0.8727\n",
            "Epoch 6/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.2874 - accuracy: 0.9274\n",
            "Epoch 7/15\n",
            "442/442 [==============================] - 18s 41ms/step - loss: 0.1883 - accuracy: 0.9547\n",
            "Epoch 8/15\n",
            "442/442 [==============================] - 18s 42ms/step - loss: 0.1355 - accuracy: 0.9689\n",
            "Epoch 9/15\n",
            "442/442 [==============================] - 18s 42ms/step - loss: 0.1102 - accuracy: 0.9725\n",
            "Epoch 10/15\n",
            "442/442 [==============================] - 18s 42ms/step - loss: 0.0905 - accuracy: 0.9784\n",
            "Epoch 11/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.0839 - accuracy: 0.9784\n",
            "Epoch 12/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.0761 - accuracy: 0.9801\n",
            "Epoch 13/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.0728 - accuracy: 0.9789\n",
            "Epoch 14/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.0696 - accuracy: 0.9791\n",
            "Epoch 15/15\n",
            "442/442 [==============================] - 19s 42ms/step - loss: 0.0669 - accuracy: 0.9792\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x121f20d03d0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuhjWYjDnzs5"
      },
      "source": [
        "seq2seq.save_weights(\"seq2seq.h5\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QOyiAOGFfnk"
      },
      "source": [
        "##Different Sampling techniques with evaluation (BLEU score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYR67YJ6j5t"
      },
      "source": [
        "We will experiment different sampling methods:\n",
        "\n",
        "1. Greedy search\n",
        "2. Top-k sampling\n",
        "3. Temperature sampling\n",
        "4. Beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiRKP807LXal"
      },
      "source": [
        "###Greedy search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3eV8WUMLbEQ",
        "outputId": "a360d02c-ce34-4f53-c04d-9b4ad1f609c9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def encode_sequence(seq):\n",
        "    encoded_seq = []\n",
        "    for j in seq.split(\" \"):\n",
        "      if j != \"\":\n",
        "        encoded_seq.append(word2idx[j])\n",
        "    encoded_seq = [encoded_seq]\n",
        "    padded_seq = pad_sequences(encoded_seq, maxlen=MAX_SEQ_LEN_ENCODER, padding=\"post\", truncating=\"pre\")\n",
        "    return padded_seq\n",
        "\n",
        "def greedy_search(predictions):\n",
        "    return np.argmax(predictions)\n",
        "\n",
        "def greedy_sampling(input_seq):\n",
        "    state = encoder_model(input_seq)\n",
        "    target_seq = np.array([[word2idx['<SOS>']]])\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state)\n",
        "        preds = output_tokens[0, -1, :]\n",
        "        sampled_token_index = greedy_search(preds)\n",
        "        sampled_char = idx2word[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        decoded_sentence += \" \"\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence.split()) > 7):\n",
        "            decoded_sentence = re.sub(\"<EOS>\", \"\",decoded_sentence)\n",
        "            break\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "    return decoded_sentence\n",
        "\n",
        "input_seq = \"Nel mezzo del cammin di nostra vita\"\n",
        "seq2seq_greedy = []\n",
        "for i in range (0, 5):\n",
        "  padded_seq = encode_sequence(input_seq)\n",
        "  decoded_sentence = greedy_sampling(padded_seq)\n",
        "  print(decoded_sentence)\n",
        "  seq2seq_greedy.append(decoded_sentence)\n",
        "  input_seq = decoded_sentence"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qualunque qualunque intende perchè natura intende se fosse \n",
            "qualunque veduta perchè muta fora fora qual fora \n",
            "qualunque qualunque qualunque intende perchè natura non nulla \n",
            "qualunque veduta divenuta perchè muta fora non fora \n",
            "qualunque veduta perchè muta fora fora qual fora \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYvuomHZMkoN"
      },
      "source": [
        "###Temperature sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHvszb0pMkUg",
        "outputId": "eeabad13-9c8f-4699-d23d-1f8e96c9f061"
      },
      "source": [
        "def temperature(predictions, temp):\n",
        "    conditional_probability = np.asarray(predictions).astype(\"float64\")\n",
        "    conditional_probability = np.log(conditional_probability) / temp\n",
        "    exp_preds = np.exp(conditional_probability)\n",
        "    conditional_probability = exp_preds / np.sum(exp_preds)\n",
        "    probs = np.random.multinomial(1, conditional_probability, 1)\n",
        "    return np.argmax(probs)\n",
        "\n",
        "def temperature_sampling(input_seq, temp):\n",
        "    state = encoder_model(input_seq)\n",
        "    target_seq = np.array([[word2idx['<SOS>']]])\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state)\n",
        "        preds = output_tokens[0, -1, :]\n",
        "        sampled_token_index = temperature(preds, temp=temp)\n",
        "        sampled_char = idx2word[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        decoded_sentence += \" \"\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence.split()) > 7):\n",
        "            decoded_sentence = re.sub(\"<EOS>\", \"\",decoded_sentence)\n",
        "            break\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "temperatures = [0.5, 1, 1.5]\n",
        "for t in temperatures:\n",
        "  print(f\"Sampling with temperature {t}:\")\n",
        "  generated_text = []\n",
        "  input_seq = \"Nel mezzo del cammin di nostra vita\"\n",
        "  for i in range (0, 5):\n",
        "    padded_seq = encode_sequence(input_seq)\n",
        "    decoded_sentence = temperature_sampling(padded_seq, t)\n",
        "    print(decoded_sentence)\n",
        "    generated_text.append(decoded_sentence)\n",
        "    input_seq = decoded_sentence\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling with temperature 0.5:\n",
            "qualunque natura natura sarebbe e questa sarebbe e \n",
            "qualunque intelletto intende tal memoria non fosse come \n",
            "qualunque veduta divenuta perchè sùbita E Beatrice pur \n",
            "qualunque veduta stessa fosse Questa veduta perchè face \n",
            "qualunque coscienza intende perchè indarno perchè indarno come \n",
            "Sampling with temperature 1:\n",
            "cavalier qualunque qualunque intende disfavillo quando fosse come \n",
            "pianta pianta addorno addorno poco stancato ma sicuri \n",
            "contraria apparenza faccendosi perch piùe seguitò Quali dispiaccia \n",
            "conforto paladino giustamente resplende anzi apprendo sì sovrano \n",
            "folle demonio chiese  \n",
            "Sampling with temperature 1.5:\n",
            "Vedrassi qualunque specchio hai ghermito e appiglia tornasse \n",
            "qualunque oriental diritto alcuna sanese Lì Schicchi fiso \n",
            "risonar proceda tale sortito come ritroviam parlar fusca \n",
            "melodia scintillando Magno ancor da sogno informante  \n",
            "percosse volontade volontade nel dimandar Navarrese sì rilevi \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-teF-4OJNA"
      },
      "source": [
        "### Top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naYXl5595Ows",
        "outputId": "23e37d60-11b1-4338-f18d-1434f9785c3b"
      },
      "source": [
        "def softmax(z):\n",
        "   return np.exp(z)/sum(np.exp(z))\n",
        "\n",
        "def top_k(predictions, k):\n",
        "    top_k_probabilities, top_k_indices= tf.math.top_k(predictions, k=k, sorted=True)\n",
        "    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "    top_k_redistributed_probability=softmax(np.log(top_k_probabilities))\n",
        "    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
        "    sampled_token = np.random.choice(top_k_indices, p=top_k_redistributed_probability)\n",
        "    return sampled_token\n",
        "\n",
        "def top_k_sampling(input_seq, k):\n",
        "    state = encoder_model(input_seq)\n",
        "    target_seq = np.array([[word2idx['<SOS>']]])\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state)\n",
        "        preds = output_tokens[0, -1, :]\n",
        "        sampled_token_index = top_k(preds, k=k)\n",
        "        sampled_char = idx2word[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        decoded_sentence += \" \"\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence.split()) > 7):\n",
        "            decoded_sentence = re.sub(\"<EOS>\", \"\",decoded_sentence)\n",
        "            break\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "input_seq = \"Nel mezzo del cammin di nostra vita\"\n",
        "seq2seq_topk = []\n",
        "for i in range (0, 5):\n",
        "  padded_seq = encode_sequence(input_seq)\n",
        "  decoded_sentence = top_k_sampling(padded_seq, 10)\n",
        "  print(decoded_sentence)\n",
        "  seq2seq_topk.append(decoded_sentence)\n",
        "  input_seq = decoded_sentence"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "umilmente solamente alcuna alcuna nota perch sarebbe questa \n",
            "qualunque qualunque intende perchè fatica ma tu vederai \n",
            "qualunque qualunque lasciasse perchè fatica rende se fosse \n",
            "qualunque veduta divenuta perchè rimane mai  \n",
            "qualunque volontà perchè torse perchè tu sazio come \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYD84E0sQFfJ"
      },
      "source": [
        "###Beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LN6Uyu3fJPW",
        "outputId": "2e9aa8ed-d6c1-4cb2-8177-9cfe74a6bd0a"
      },
      "source": [
        "def get_candidates(target_seq, state, k):\n",
        "    output_tokens, state = decoder_model(target_seq, state)\n",
        "    preds = output_tokens[0, -1, :]\n",
        "    top_k_probabilities, top_k_indices= tf.math.top_k(preds, k=k, sorted=True)\n",
        "    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "    top_k_probabilities = np.asarray(top_k_probabilities).astype(\"float32\")\n",
        "    return top_k_indices, top_k_probabilities\n",
        "    \n",
        "\n",
        "def beam_search_inference(input_seq, k=3, max_words=5):\n",
        "    state = encoder_model(input_seq)\n",
        "    scores = [[(\"<SOS>\", 1.0)]]\n",
        "    target_seq = np.array([[word2idx['<SOS>']]])\n",
        "    for c in range (0, max_words):\n",
        "      for i in range (len(scores[c])):\n",
        "        k_scores = []\n",
        "        for seq, score in scores[c]:\n",
        "          target_seq = np.array([[word2idx[seq.split()[-1]]]])\n",
        "          top_k_indices, top_k_probabilities = get_candidates(target_seq, state, k)\n",
        "          for j in range (0, k):\n",
        "            inner_sentence = seq + \" \" + idx2word[top_k_indices[j]]\n",
        "            inner_score = score+ top_k_probabilities[j]\n",
        "            inner_tup = (inner_sentence, inner_score)\n",
        "            k_scores.append(inner_tup)\n",
        "      scores.append(k_scores)\n",
        "    final_candidates = np.array([s for c,s in scores[-1]])\n",
        "    max_prob_idx = np.argmax(final_candidates)\n",
        "    final_seq = scores[-1][max_prob_idx][0]\n",
        "    final_seq = re.sub(\"<SOS> \", \"\",final_seq)\n",
        "    return final_seq\n",
        "\n",
        "\n",
        "input_seq = \"Nel mezzo del cammin di nostra vita\"\n",
        "seq2seq_beam = []\n",
        "for i in range (0, 5):\n",
        "  padded_seq = encode_sequence(input_seq)\n",
        "  decoded_sentence = beam_search_inference(padded_seq, k=2, max_words=5)\n",
        "  print(decoded_sentence)\n",
        "  seq2seq_beam.append(decoded_sentence)\n",
        "  input_seq = decoded_sentence"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pianto piangendo piangendo piangendo qualunque\n",
            "silenzio benedetto benedetto benedetto benedetto\n",
            "benedetto benedetto benedetto benedetto benedetto\n",
            "maladetto diserto diserto diserto diserto\n",
            "benedetto benedetto benedetto benedetto benedetto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "aTjEFkj-JcLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "def bleu(data, generated, weights=(0.25,0.25,0.25,0.25)):\n",
        "  cc = SmoothingFunction()\n",
        "  references = [seq.text.split() for seq in data]\n",
        "  hypothesis = [seq.split() for seq in generated]\n",
        "  scores = []\n",
        "  for i in range(len(hypothesis)):\n",
        "    bleu_score = sentence_bleu(references, hypothesis[i], weights=weights, smoothing_function=cc.method4)\n",
        "    scores.append(bleu_score)\n",
        "  return sum(scores)/len(scores)"
      ],
      "metadata": {
        "id": "fnr6EamBJyU3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import nltk\n",
        "from nltk.translate import meteor\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def text_meteor_score(sequences, generated):\n",
        "  seq_list = [seq.text for seq in sequences]\n",
        "  text_score = []\n",
        "  best = []\n",
        "  for i in generated:\n",
        "    sequence_score = []\n",
        "    for j in seq_list:\n",
        "      sequence_score.append(round(meteor(references=[word_tokenize(j)], hypothesis=word_tokenize(i)), 4))\n",
        "    max_value = max(sequence_score)\n",
        "    max_index = sequence_score.index(max_value)\n",
        "    best.append(seq_list[max_index])\n",
        "    text_score.append(max(sequence_score))\n",
        "  return sum(text_score)/len(text_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SBGe202JmiZ",
        "outputId": "c79f5c02-b2e7-454a-b67a-91cee3181ff5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\francesco.farinola\\AppData\\Roaming\\nltk_data.\n",
            "[nltk_data]     ..\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\francesco.farinola\\AppData\\Roaming\\nltk_data.\n",
            "[nltk_data]     ..\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Char-level temperature sampling"
      ],
      "metadata": {
        "id": "whkIR6s9JiB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_generation = char_level_generation.split(\" \\n \")[1:-1]\n",
        "for i in char_generation:\n",
        "  print(i)\n",
        "print(f\"BLEU score: {bleu(sequences, char_generation)}\")\n",
        "print(f\"METEOR score: {text_meteor_score(sequences, char_generation)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIwKdAaN1z9S",
        "outputId": "8f08dbd3-c122-4c40-f396-dd0f5f146679"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tenean la testa e ancor tutto 'l casso\n",
            "e di costoro assai riconobb' io\n",
            "con più color che sono in terra\n",
            "tutti sviati dietro al malo essemplo\n",
            "già si solea con le spade farne\n",
            "sorte pria che sia la corda queta\n",
            "così corremmo nel vocabo prima\n",
            "che le più alte cime più percuote\n",
            "e ciò non fa d'onor poco argomento\n",
            "per che se tu a la virtù che vole\n",
            "freno a dimandar più tardo\n",
            "intra due cibi distanti e moventi\n",
            "d'un modo e l'altro insieme\n",
            "l'un de l'altro giacea e qual carpone\n",
            "BLEU score: 0.8608256255637147\n",
            "METEOR score: 0.8402071428571427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Greedy search"
      ],
      "metadata": {
        "id": "SyDXZYsZKGfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in seq2seq_greedy:\n",
        "  print(i)\n",
        "print(f\"BLEU score: {bleu(sequences, seq2seq_greedy)}\")\n",
        "print(f\"METEOR score: {text_meteor_score(sequences, seq2seq_greedy)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCQmELP-KGNa",
        "outputId": "b5040867-048b-42b0-aed5-be3d5653c8d3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qualunque qualunque intende perchè natura intende se fosse \n",
            "qualunque veduta perchè muta fora fora qual fora \n",
            "qualunque qualunque qualunque intende perchè natura non nulla \n",
            "qualunque veduta divenuta perchè muta fora non fora \n",
            "qualunque veduta perchè muta fora fora qual fora \n",
            "BLEU score: 0.09902248405257925\n",
            "METEOR score: 0.28418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seq2Seq Temperature sampling"
      ],
      "metadata": {
        "id": "AtbHI4N9KMFo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nalL6UAoT8lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e2badb-5a2b-48e5-8cef-9474a2cef13e"
      },
      "source": [
        "seq2seq_temperature = []\n",
        "input_seq = \"Nel mezzo del cammin di nostra vita\"\n",
        "for i in range (0, 5):\n",
        "  padded_seq = encode_sequence(input_seq)\n",
        "  decoded_sentence = temperature_sampling(padded_seq, 1)\n",
        "  print(decoded_sentence)\n",
        "  seq2seq_temperature.append(decoded_sentence)\n",
        "  input_seq = decoded_sentence\n",
        "\n",
        "print(f\"BLEU score: {bleu(sequences, seq2seq_temperature)}\")\n",
        "print(f\"METEOR score: {text_meteor_score(sequences, seq2seq_temperature)}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qualunque qualunque pogna mente come infiora partìne  \n",
            "qualunque bisogna quando non troveria tanto come superbe \n",
            "qualunque intende qualunque avresti concetto lega sì contento \n",
            "qualunque esperienza labbia questo vole degno degno  \n",
            "qualunque intelletto prende allora vole ghiotta questo capestro \n",
            "BLEU score: 0.07108218095587185\n",
            "METEOR score: 0.23746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seq2Seq Top-k sampling"
      ],
      "metadata": {
        "id": "xHvm3TTCKi2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in seq2seq_topk:\n",
        "  print(i)\n",
        "print(f\"BLEU score: {bleu(sequences, seq2seq_topk)}\")\n",
        "print(f\"METEOR score: {text_meteor_score(sequences, seq2seq_topk)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCtVwTCE_wxG",
        "outputId": "f782a60f-80a8-4363-b766-d86a58300247"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "umilmente solamente alcuna alcuna nota perch sarebbe questa \n",
            "qualunque qualunque intende perchè fatica ma tu vederai \n",
            "qualunque qualunque lasciasse perchè fatica rende se fosse \n",
            "qualunque veduta divenuta perchè rimane mai  \n",
            "qualunque volontà perchè torse perchè tu sazio come \n",
            "BLEU score: 0.08878141361231363\n",
            "METEOR score: 0.28206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seq2Seq Beam search"
      ],
      "metadata": {
        "id": "1RvpwB1KKlrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in seq2seq_beam:\n",
        "  print(i)\n",
        "print(f\"BLEU score: {bleu(sequences, seq2seq_beam)}\")\n",
        "print(f\"METEOR score: {text_meteor_score(sequences, seq2seq_beam)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn4ZMuv9-1ty",
        "outputId": "67900166-ba07-4dd1-de6e-f08022c1af9f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pianto piangendo piangendo piangendo qualunque\n",
            "silenzio benedetto benedetto benedetto benedetto\n",
            "benedetto benedetto benedetto benedetto benedetto\n",
            "maladetto diserto diserto diserto diserto\n",
            "benedetto benedetto benedetto benedetto benedetto\n",
            "BLEU score: 0.0519903281559748\n",
            "METEOR score: 0.12884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ONFmxK40wcs"
      },
      "source": [
        "# Extra - Term frequency?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUPQhf7n862t",
        "outputId": "66eaadf5-1901-4467-e45c-609a5b33bb8a"
      },
      "source": [
        "\"\"\"from collections import Counter\n",
        "\n",
        "def frequent_words(df):\n",
        "  corpus = [text for text in df.Text]\n",
        "  corpus = reduce(lambda x,y: x+y, corpus)\n",
        "  tokens = nlp(corpus)\n",
        "  df_list = [word.text for word in tokens]\n",
        "  word_counts = Counter(df_list)\n",
        "  del word_counts[\"\\n\"]\n",
        "  most_common = word_counts.most_common(50)\n",
        "  words = list(zip(*most_common))[0]\n",
        "  counts = list(zip(*most_common))[1]\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  plt.bar(words, counts)\n",
        "  plt.show()\n",
        "  return word_counts\n",
        "\n",
        "word_counts = frequent_words(df)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from collections import Counter\\n\\ndef frequent_words(df):\\n  corpus = [text for text in df.Text]\\n  corpus = reduce(lambda x,y: x+y, corpus)\\n  tokens = nlp(corpus)\\n  df_list = [word.text for word in tokens]\\n  word_counts = Counter(df_list)\\n  del word_counts[\"\\n\"]\\n  most_common = word_counts.most_common(50)\\n  words = list(zip(*most_common))[0]\\n  counts = list(zip(*most_common))[1]\\n  plt.figure(figsize=(20, 10))\\n  plt.bar(words, counts)\\n  plt.show()\\n  return word_counts\\n\\nword_counts = frequent_words(df)'"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuxBZrRc3jFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ed425b-6b84-44b4-9b9c-3396173b6805"
      },
      "source": [
        "\"\"\"def compute_tf(word_counts):\n",
        "  n_tokens = np.sum(list(word_counts.values()))\n",
        "  for k in word_counts:\n",
        "    word_counts[k] = 1 + np.log10(word_counts[k]) \n",
        "  return word_counts\n",
        "\n",
        "\n",
        "term_freq = compute_tf(word_counts)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def compute_tf(word_counts):\\n  n_tokens = np.sum(list(word_counts.values()))\\n  for k in word_counts:\\n    word_counts[k] = 1 + np.log10(word_counts[k]) \\n  return word_counts\\n\\n\\nterm_freq = compute_tf(word_counts)'"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnBjg2yO9HA1"
      },
      "source": [
        "def get_tf_matrix(input):\n",
        "    tf_matrix = np.zeros(shape=(input.shape[0], input.shape[1]))\n",
        "    for i, sequence in enumerate(input):\n",
        "      for j, word in enumerate(sequence):\n",
        "        tf_matrix[i, j] = term_freq[idx2word[word]]\n",
        "    return tf_matrix\n",
        "      \n",
        "tf_matrix = get_tf_matrix(decoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}